#+LATEX_HEADER: \usepackage[margin=0.55in]{geometry}

* Preliminary Results

Using several measures of accuracy and error, results are displayed for the same gradient boosting model (i.e. using the same input parameters) for each of the four data sets earlier defined, for each of the  the lag values one to five.

These results all stem from the LOCF imputation method.


* Two Models

** Model 1: mstop = 5000, nu = 0.005
 

*** Table 1

| Lag | Subset | Pred. Acc. (%) | MAE     | MSE     | RMSE    | mstop avg. | mstop SD |
|-----+--------+----------------+---------+---------+---------+------------+----------|
| <c> | <c>    | <c>            | <c>     | <c>     | <c>     |            |          |
| 1   | Macro  | 81.10          | 2.98e-3 | 1.67e-5 | 4.09e-3 |       1369 |     1052 |
| 1   | SA_all | 67.07          | 5.02e-3 | 4.59e-5 | 6.78e-3 |        414 |      469 |
| 1   | SA_avg | 69.81          | 4.84e-3 | 4.36e-5 | 6.60e-3 |        398 |      379 |
| 1   | Mix    | 82.62          | 2.94e-3 | 1.64e-5 | 4.05e-3 |        870 |      733 |
|-----+--------+----------------+---------+---------+---------+------------+----------|
| 2   | Macro  | 80.31          | 2.93e-3 | 1.65e-5 | 4.06e-3 |       1171 |      933 |
| 2   | SA_all | 63.97          | 5.01e-3 | 4.54e-5 | 6.74e-3 |        387 |      516 |
| 2   | SA_avg | 69.62          | 4.84e-3 | 4.31e-5 | 6.57e-5 |        358 |      359 |
| 2   | Mix    | 82.90          | 2.94e-3 | 1.68e-5 | 4.10e-3 |        715 |      490 |
|-----+--------+----------------+---------+---------+---------+------------+----------|
| 3   | Macro  | 81.04          | 2.92e-3 | 1.62e-5 | 4.02e-3 |       1003 |      808 |
| 3   | SA_all | 64.83          | 5.01e-3 | 4.55e-5 | 6.75e-3 |        363 |      433 |
| 3   | SA_avg | 70.03          | 4.85e-3 | 4.27e-5 | 6.54e-3 |        349 |      378 |
| 3   | Mix    | 83.33          | 2.92e-3 | 1.65e-5 | 4.06e-3 |        602 |      334 |
|-----+--------+----------------+---------+---------+---------+------------+----------|
| 4   | Macro  | 80.70          | 2.92e-3 | 1.62e-5 | 4.03e-3 |        871 |      689 |
| 4   | SA_all | 62.63          | 5.07e-3 | 4.58e-5 | 6.77e-3 |        351 |      537 |
| 4   | SA_avg | 70.14          | 4.89e-3 | 4.30e-5 | 6.55e-3 |        408 |      555 |
| 4   | Mix    | 82.70          | 2.93e-3 | 1.67e-5 | 4.08e-3 |        543 |      244 |
|-----+--------+----------------+---------+---------+---------+------------+----------|
| 5   | Macro  | 81.90          | 2.90e-3 | 1.60e-5 | 4.00e-3 |        814 |      615 |
| 5   | SA_all | 63.19          | 5.00e-3 | 4.57e-5 | 6.76e-3 |        303 |      414 |
| 5   | SA_avg | 68.10          | 4.85e-3 | 4.25e-5 | 6.52e-3 |        370 |      444 |
| 5   | Mix    | 82.67          | 2.92e-3 | 1.67e-5 | 4.09e-3 |        518 |      214 |
|-----+--------+----------------+---------+---------+---------+------------+----------|

** Model 2: mstop = 2000, nu = 0.05


*** Table 2

| Lag | Subset | Pred. Acc. (%) | MAE     | MSE     | RMSE    | mstop avg. | mstop SD |
|-----+--------+----------------+---------+---------+---------+------------+----------|
| <c> | <c>    | <c>            | <c>     | <c>     | <c>     |            |          |
| 1   | Macro  | 80.95          | 2.99e-3 | 1.67e-5 | 4.09e-3 | 151        | 192      |
| 1   | SA_all | 66.92          | 5.02e-3 | 4.59e-5 | 6.77e-3 | 41         | 46       |
| 1   | SA_avg | 69.51          | 4.83e-3 | 4.35e-5 | 6.59e-3 | 39         | 37       |
| 1   | Mix    | 82.62          | 2.93e-3 | 1.64e-5 | 4.05e-3 | 91         | 116      |
|-----+--------+----------------+---------+---------+---------+------------+----------|
| 2   | Macro  | 80.46          | 2.93e-3 | 1.64e-5 | 4.05e-3 | 119        | 128      |
| 2   | SA_all | 64.73          | 5.01e-3 | 4.54e-5 | 6.74e-3 | 42         | 117      |
| 2   | SA_avg | 69.62          | 4.85e-3 | 4.32e-5 | 6.57e-3 | 37         | 62       |
| 2   | Mix    | 82.75          | 2.95e-3 | 1.69e-5 | 4.11e-3 | 70         | 52       |
|-----+--------+----------------+---------+---------+---------+------------+----------|
| 3   | Macro  | 80.73          | 2.93e-3 | 1.62e-5 | 4.03e-3 | 104        | 129      |
| 3   | SA_all | 65.29          | 5.01e-3 | 4.54e-5 | 6.74e-3 | 38         | 87       |
| 3   | SA_avg | 70.03          | 4.86e-3 | 4.28e-5 | 6.54e-3 | 35         | 42       |
| 3   | Mix    | 83.64          | 2.91e-3 | 1.64e-5 | 4.05e-3 | 59         | 35       |
|-----+--------+----------------+---------+---------+---------+------------+----------|
| 4   | Macro  | 80.55          | 2.92e-3 | 1.62e-5 | 4.02e-3 | 91         | 130      |
| 4   | SA_all | 63.40          | 5.07e-3 | 4.58e-5 | 6.77e-3 | 38         | 106      |
| 4   | SA_avg | 69.98          | 4.90e-3 | 4.31e-5 | 6.56e-3 | 42         | 66       |
| 4   | Mix    | 82.54          | 2.92e-3 | 1.66e-5 | 4.07e-3 | 53         | 25       |
|-----+--------+----------------+---------+---------+---------+------------+----------|
| 5   | Macro  | 81.60          | 2.90e-3 | 1.61e-5 | 4.01e-3 | 81         | 74       |
| 5   | SA_all | 63.96          | 5.01e-3 | 4.58e-5 | 6.77e-3 | 35         | 110      |
| 5   | SA_avg | 67.94          | 4.85e-3 | 4.26e-5 | 6.52e-3 | 37         | 46       |
| 5   | Mix    | 82.67          | 2.93e-3 | 1.68e-5 | 4.10e-3 | 50         | 19       |
|-----+--------+----------------+---------+---------+---------+------------+----------|

** Initial inspection

We define a measurement of error and success of a prediction as the _sign accuracy_. This is a simply test with binary response. Did we predict the sign of the market return correctly? If yes, we record '1', of not, '0'. The column _Predictive accuracy_ is then defined as the percentage of correct predictions measured by the sign accuracy. The numerical descrepancy between the two values is not taken into consideration.

Inspecting first the predictive accuracy in Table 1 for the subsets _Macro_, _SA_all_ and _SA_avg_, we see that every model unequivocally gains some traction on the data, each model performing better than a random selection producing 50% accuracy for a binary response.

We notice further that the traditional factors grouped in the _Macro_ data subset offer the greatest level accuracy, being consistently over 80%. This is a results that one would come to expect.
The two variations of the sentiment analysis data perform less convincingly, nonetheless they consistently return a net positive contribution, beating 50% in every case.

Now we have established the _Macro_ data set as the superior, we can compare it to the _Mix_ data set.
Excitingly, this data set outperforms the _Macro_ data set across the board - on average by *1.83 %* for Model 1 and *1.99 %* for Model 2.


Another feature to be noticed from both Table 1 and Table 2 is that the Mix data set not only exhibited a greater predictive accuracy, it also managed to do so in less iterations during the gradient boosting. This 'faster descent' can be explained by several features of the data set. First of all in comparison to the 

A final important conclusion to be drawn from these results, is that the errors associated with the predictions of the _Macro_ and the _Mix_ subsets are very similar. [Sum up the number that are smaller/larger].



Now turning our attention to the last two columns of the tables, we can compare the critical model parameter: *mstop*.
Model 1 has a finer granularity and so makes the gradient descent at a slower pace.
This means higher computational cost due to more iterations being required to reach the _local_ minima during the gradient descent. This is reflected in the column *mstop avg*, with many cases averaging an mstop value of more than 500.

Comparing this to the average mstop values obtained when using the same data, but with a larger shrinkage value (i.e. a higher granularity), we see that using a shrinkage value ten-fold smaller produces values almost linearly scalable, the mstp values being approximately ten times smaller.


