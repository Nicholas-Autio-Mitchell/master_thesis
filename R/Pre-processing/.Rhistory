                gsub("^ ", "", .) %>%
                gsub(" $", "", .)})
library("dplyr")
library("tm")
clean_file <-  sapply(dirty_file, function(x) {
            gsub("&amp;", "&", x) %>%
                gsub("http\\S+\\s*", "", .) %>%
                gsub("[^[:alpha:][:space:]&:\']", "", .) %>%
                stripWhitespace() %>%
                gsub("^ ", "", .) %>%
                gsub(" $", "", .)})
clean_file
encoding(dirty_file)
Encoding(dirty_file)
Encoding(dirty_file[1,1])
dirty_file[1,1]
x <- dirty_file[1,1]
x
Encoding(x)
Encoding(x) <- "UTF-8"
Encoding(x)
x
dirty_file[1,10]
dim(dirty_file)
dirty_file[10,1]
dirty_file[100,1]
dirty_file[110,1]
dirty_file[2,1]
clean_file[2,1]
clean_file <-  sapply(dirty_file, function(x) {

            gsub("&amp;", "&", x) %>%
                gsub("http\\S+\\s*", "", .) %>%
                gsub("[^[:alpha:][:space:]&:\']", "", .) %>%
                stripWhitespace() %>%
                gsub("^ ", "", .) %>%
                gsub(" $", "", .)})
x
x <- dirty_file[2:5,1]
x
y <-  sapply(x, function(x) {
            gsub("&amp;", "&", x) %>%
                gsub("http\\S+\\s*", "", .) %>%
                gsub("[^[:alpha:][:space:]&:\']", "", .) %>%
                stripWhitespace() %>%
                gsub("^ ", "", .) %>%
                gsub(" $", "", .)})
y
dirty_file[1:5,1]
x <- dirty_file[1:5,1]
x
gsub("http://", "", x)
?gsub
clean_file <-  sapply(dirty_file, function(x) {
            gsub("&amp;", "&", ., useBytes = TRUE) %>%
                gsub("http\\S+\\s*", "", ., useBytes = TRUE) %>%
                gsub("[^[:alpha:][:space:]&:\']", "", ., useBytes = TRUE) %>%
                stripWhitespace() %>%
                gsub("^ ", "", .) %>%
                gsub(" $", "", .)})
clean_file <-  sapply(dirty_file, function(x) {
            gsub("&amp;", "&", x, useBytes = TRUE) %>%
                gsub("http\\S+\\s*", "", ., useBytes = TRUE) %>%
                gsub("[^[:alpha:][:space:]&:\']", "", ., useBytes = TRUE) %>%
                stripWhitespace() %>%
                gsub("^ ", "", .) %>%
                gsub(" $", "", .)})
clean_file
write.csv(clean_file, "whatever.txt")
?write.csv
write.csv(clean_file, "whatever.txt", fileEncoding = "UTF-8")
write.csv(clean_file, "whatever.txt", fileEncoding = "latin1")
write.csv(clean_file, "whatever.txt", fileEncoding = "latin-1")
write.csv(clean_file, "whatever.txt", fileEncoding = "Latin-1")
write.csv(clean_file, "whatever.txt", fileEncoding = "ASCII")
warnings()
curr_file = read.csv("tester.txt")
curr_file = read.csv("tester.txt")
lsos()
library("multilevelPSA")
lsos()
rm(curr_file)
ls()
rm(ls())
rm(ls = ls())
curr_file
=======
install.packages("vars")
install.packages("MSBVAR")
install.packages("SuppDists")
library("SuppDists", lib.loc="/Library/Frameworks/R.framework/Versions/3.0/Resources/library")
xx<-rnorm(500)
parms<-JohnsonFit(xx)
sJohnson(parms)
plot(function(xx)dJohnson(xx,parms),-2,2)
pJohnson(1,parms)
parms2<-JohnsonFit(rexp(50))
qJohnson(p=0.5,list(parms,parms2))
xx
xx<-rnorm(10000)
parms<-JohnsonFit(xx)
sJohnson(parms)
plot(function(xx)dJohnson(xx,parms),-2,2)
pJohnson(1,parms)
parms2<-JohnsonFit(rexp(50))
qJohnson(p=0.5,list(parms,parms2))
xx
parms<-JohnsonFit(xx)
sJohnson(parms)
plot(function(xx)dJohnson(xx,parms),-2,2)
pJohnson(1,parms)
parms2<-JohnsonFit(rexp(50))
qJohnson(p=0.5,list(parms,parms2))
n <- 1000
mean.eq <- 0.001
sd.eq   <- 0.02
contamination.ratio <- 0.01
contaminated.sd <- 6
cf1 <- 1
cf2 <- 0.8
x <- numeric(n)
contaminatedN <- round(n*contamination.ratio)
contaminated.idx <- sample(1:n,contaminatedN,replace=FALSE)
x[contaminated.idx] <- rnorm(contaminatedN,mean.eq,contaminated.sd)/100
sd.d <- numeric(n)
for (i in 1:n) {
if (is.na(match(i,contaminated.idx))) {
sd.d[i] <- sum( (cf1/abs(i-contaminated.idx)^cf2) * abs(x[contaminated.idx]) )
x[i]    <- rnorm( 1,0,sd.eq + ifelse(sd.d[i]>sd.eq,sd.d[i],0) )
} else sd.d[i] <- sd.d[i-1]
}
par(mfrow=c(3,1))
plot(cumsum(x),type="l")
plot(x,type="l")
plot(sd.d,type="l")
abline(h=sd.eq,col="Red",lty="dotted")
par(mfrow=c(1,1))
contiminated.test <- 0.3
Apple <- getOptionChain("AAPL", Exp = NULL, src="yahoo")
library("quantmod", lib.loc="/Library/Frameworks/R.framework/Versions/3.0/Resources/library")
Apple <- getOptionChain("AAPL", Exp = NULL, src="yahoo")
Apple
y = getSymbols("YHOO")
View(YHOO)
chartSeries(YHOO)
chartSeries(YHOO, subset='last 4 months')
chartSeries(YHOO,theme=chartTheme('white'))
chartSeries(YHOO,TA=c(addVo(),addBBands()))
plot(Apple)
plot(Apple, 1:1000)
length(Apple)
length(Apple.1)
length(Apple(1))
x = 1:3
x
x = 1:2:10
x
data1 = read.csv(file.choose(), header = T)
str(Apple)
sum(Apple)
sum("Apple")
summary("Apple")
fix("Apple")
view("Apple")
summary(Apple)
library("GLDEX", lib.loc="/Library/Frameworks/R.framework/Versions/3.0/Resources/library")
chartSeries(YHOO,TA=c(addVo(),addBBands()))
read.csv(file.choose())
read.csv(file.choose(), headers=T)
read.csv(file.choose(), header=TRUE)
funds <- read.csv(file.choose(), header=TRUE);
View(funds)
delete(funds[,1])
rm(funds[,1])
rm(funds[X])
rm(funds(X))
funds <- funds[,-1]
View(funds)
fun.moments.r(funds(,4))
funds(4)
funds[4]
fun.moments.r(funds[4])
l = funds[4]
View(l)
p = l[-1]
View(p)
k = l[-1,]
View(p)
View(p)
rm(p)
p = l(-1,)
p = l[-1,]
p
p''
transpose(p)
fun.moments.r(l, normalise=Y)
fun.moments.r(l, normalise='Y')
l <- t(l)
l
View(l)
p
fun.moments.r(l, normalise='Y')
l <- as.vector(l)
fun.moments.r(l, normalise='Y')
View(funds)
CTA = funds[5]
View(CTA)
CTA <- as.vector(CTA)
View(CTA)
fun.moments.r(CTA, normalise ='Y')
CTA(1)
CTA[1]
CTA[1,1]
CTA[-(1,1)]
CTA[-(1;1)]
CTA[-1]
View(CTA)
CTB <- CTA[-1]
View(CTB)
rm(CTB)
fun.moments.r(CTA)
CTM <- as.matrix(CTA)
View(CTM)
View(CTM)
fun.moments.r(CTM)
CTM <- CTM[-1]
CTM
fun.moments.r(CTM)
as.numeric(CTM)
fun.moments.r(CTM)
CTM
CTM <- as.numeric(CTM)
fun.moments.r(CTM)
moments <- fun.moments.r(CTM, normalise='Y')
moments
CTA_GLD <- fun.data.fit.mm(CTM)
CTA_GLD
fun.plot.fit(fit.obj = CTA_GLD, data = CTM, nclass = 100, param = c("rs", "fmkl"), xlab = "Returns")
fun.plot.fit(fit.obj = CTA_GLD, data = CTM, nclass = 150, param = c("rs", "fmkl"), xlab = "Returns", legend='F')
fun.plot.fit(fit.obj = CTA_GLD, data = CTM, nclass = 150, param = c("rs", "fmkl"), xlab = "Returns", name="F")
fun.plot.fit(fit.obj = CTA_GLD, data = CTM, nclass = 150, param = c("rs", "fmkl"), xlab = "Returns", name="")
fun.plot.fit(fit.obj = CTA_GLD, data = CTM, nclass = 150, param = c("rs", "fmkl"), xlab = "Returns", name="XX")
fun.plot.fit(fit.obj = CTA_GLD, data = CTM, nclass = 150, param = c("", ""), xlab = "Returns", name="")
fun.plot.fit(fit.obj = CTA_GLD, data = CTM, nclass = 150, xlab = "Returns", name="")
fun.plot.fit(fit.obj = CTA_GLD, data = CTM, nclass = 150, param = c("rs", "fmkl"), xlab = "Returns", main="Generlies Lambda Distributions")
fun.plot.fit(fit.obj = CTA_GLD, data = CTM, nclass = 150, param = c("rs", "fmkl"), xlab = "Returns", main="Generalised Lambda Distributions")
legend("bottom", c("IM", "IBD", "1R", "2R"), xpd = TRUE, horiz = TRUE, inset = c(0,0), bty = "n", pch = c(4, 2, 15, 19), col = 1:4, cex = 2)
par(fig = c(0, 1, 0, 1), oma = c(0, 0, 0, 0), mar = c(0, 0, 0, 0), new = TRUE)
plot(0, 0, type = "n", bty = "n", xaxt = "n", yaxt = "n")
legend("bottom", c("IM", "IBD", "1R", "2R"), xpd = TRUE, horiz = TRUE, inset = c(0, 0), bty = "n", pch = c(4, 2, 15, 19), col = 1:4, cex = 2)
fun.plot.fit(fit.obj = CTA_GLD, data = CTM, nclass = 150, param = c("rs", "fmkl"), xlab = "Returns", main="Generalised Lambda Distributions")
fun.plot.fit(fit.obj = CTA_GLD, data = CTM, nclass = 150, param = c("rs", "fmkl"), xlab = "Returns", main="Generalised Lambda Distributions")
lambda_params_rs <- CTA_GLD[, 1]
lambda1_rs <- lambda_params_rs[1]
lambda2_rs <- lambda_params_rs[2]
lambda3_rs <- lambda_params_rs[3]
lambda4_rs <- lambda_params_rs[4]
lambda_params_fmkl <- CTA_GLD[, 2]
lambda1_fmkl <- lambda_params_fmkl[1]
lambda2_fmkl <- lambda_params_fmkl[2]
lambda3_fmkl <- lambda_params_fmkl[3]
lambda4_fmkl <- lambda_params_fmkl[4]
CTA_GLD
rs_sample <- rgl(10000000, lambda1=lambda1_rs, lambda2=lambda2_rs, lambda3=lambda3_rs, lambda4=lambda4_rs, param="rs")
fun.moments.r(rs_sample, normalise="Y")
fun.moments.r(CTM, normalise="Y")
??"transpose"
install.packages(c("boot", "class", "cluster", "codetools", "foreign", "KernSmooth", "lattice", "manipulate", "MASS", "Matrix", "mgcv", "nlme", "nnet", "rpart", "spatial", "survival"))
install.packages("twitteR")
library("twitteR", lib.loc="/Library/Frameworks/R.framework/Versions/3.2/Resources/library")
install.packages("quantmod")
help
help GLDEX
R.home(component = "home")
path.expand("~")
install.packages("ISLR")
R.home(component = "home")
clear
installed.packages("RCurl")
install.packages("RCurl")
require(twitteR)
require(RCurl)
library("twitteR", lib.loc="/Library/Frameworks/R.framework/Versions/3.2/Resources/library")
library("base64enc", lib.loc="/Library/Frameworks/R.framework/Versions/3.2/Resources/library")
library("curl", lib.loc="/Library/Frameworks/R.framework/Versions/3.2/Resources/library")
library("httr", lib.loc="/Library/Frameworks/R.framework/Versions/3.2/Resources/library")
library("jsonlite", lib.loc="/Library/Frameworks/R.framework/Versions/3.2/Resources/library")
library("plyr", lib.loc="/Library/Frameworks/R.framework/Versions/3.2/Resources/library")
library("RCurl", lib.loc="/Library/Frameworks/R.framework/Versions/3.2/Resources/library")
library("ROAuth", lib.loc="/Library/Frameworks/R.framework/Versions/3.2/Resources/library")
consumerKey <- 'aRE1YNfXXaw15M4MIs6w9JpY9'
consumerSecret <- 'dluHwkkzokjSusKGD6soLCSYTVUTtYEi326xDuliyHr36xpCtL'
accessToken <- '3402568396-mygMgTzp0tr3on7ySLRp4PUesc1odyELnkznnsy'
accessTokenSecret <- 'OzgNs8hh2HWPdRYoFwgYiK3Hs8OzOtVNoy5eGFcgaMofs'
setup_twitter_oauth(consumerKey, consumerSecret, accessToken, accessTokenSecret)
setup_twitter_oauth(consumerKey, consumerSecret, accessToken, accessTokenSecret)
setup_twitter_oauth(consumerKey, consumerSecret, accessToken, accessTokenSecret)
remove(consumerKey, consumerSecret, accessToken, accessTokenSecret, 0)
remove(consumerKey, consumerSecret, accessToken, accessTokenSecret)
consumerKey <- 'aRE1YNfXXaw15M4MIs6w9JpY9'
consumerSecret <- 'dluHwkkzokjSusKGD6soLCSYTVUTtYEi326xDuliyHr36xpCtL'
accessToken <- '3402568396-mygMgTzp0tr3on7ySLRp4PUesc1odyELnkznnsy'
accessTokenSecret <- 'OzgNs8hh2HWPdRYoFwgYiK3Hs8OzOtVNoy5eGFcgaMofs'
setup_twitter_oauth(consumerKey, consumerSecret, accessToken, accessTokenSecret)
setup_twitter_oauth(consumerKey, consumerSecret, accessToken, accessTokenSecret)
searchTerms <- "federal reserve"
maxTweets <- 3000
startDate <- '2015-01-01'
endDate <- '2015-06-30'
language <- "en"
resultSort <- 'recent'
tweets <- searchTwitter(searchTerms, n = maxTweets, since = startDate, lang = language, resultType = resultSort)
View(tweets)
tweets_text <- sapply(tweets, function(x) x$getText())
tweets_dates <- lapply(tweets, function(x) x$getCreated())
Remove all upper case characters
tweets_clean <- tolower(tweets_clean)
tweets_clean <- gsub("&amp;", "&", tweets_clean)
tweets_clean <- gsub("[^[:alnum:][:space:]&-]", "", tweets_clean)
tweets_clean <- sapply(tweets_text, function(row) iconv(row, "latin1", "ASCII", sub=""), USE.NAMES = FALSE)
tweets_clean <- tolower(tweets_clean)
tweets_clean <- gsub("&amp;", "&", tweets_clean)
tweets_clean <- gsub("[^[:alnum:][:space:]&-]", "", tweets_clean)
to_retain <- grep(pattern = searchTerms, x = tweets_clean, ignore.case = TRUE, useBytes = TRUE)
if (length(to_retain) > 0.7 * length(tweets_clean)) {
tweets_clean <- tweets_clean[to_retain]
} else {
message("More than 30% of tweets did not contain any of the search terms. None were removed")
}
tweets_dates <- tweets_dates[to_retain]
to_remove_foreign <- grep('abre', tweets_clean, ignore.case = TRUE)
if (length(to_remove_foreign) > 0) {
tweets_clean <- tweets_clean[-(to_remove_foreign)]
} else {
message("It appears all tweets are clear of select foreign words - therefore none were removed")
}
tweets_dates <- tweets_dates[-(to_remove_foreign)]
tweets_clean <- gsub("http(.*)\\b+", "", tweets_clean)
tweets_clean <- stripWhitespace(tweets_clean)
tweets_clean <- gsub("^ ", "", tweets_clean)
tweets_clean <- gsub(" $", "", tweets_clean)
tweets_clean <- gsub("[ |\t]{2,}", "", tweets_clean)
retweet_positions <- grep(pattern = "^rt", x = tweets_clean, ignore.case = FALSE)
retweet_percentage = length(retweet_positions) / length(tweets_clean) * 100
tweets_table <- as.data.table(tweets_clean)
library("data.table", lib.loc="/Library/Frameworks/R.framework/Versions/3.2/Resources/library")
tweets_table <- as.data.table(tweets_clean)
tweets_table
dates_table <- as.data.table(tweets_dates, keep.rownames = TRUE)
dates <- as.character.Date(x = dates_table[1,])
dim(dates)
dates_transposed <- t(dates)
dates <- as.character.Date(x = dates_table[1,])
dim(dates)
str(tweets)
mode(tweets)
table1 <- as.data.table(tweets)
View(table1)
save.image("~/Desktop/old_tweets_to_test.RData")
View(table1)
table1
consumerKey <- 'aRE1YNfXXaw15M4MIs6w9JpY9'
consumerSecret <- 'dluHwkkzokjSusKGD6soLCSYTVUTtYEi326xDuliyHr36xpCtL'
accessToken <- '3402568396-mygMgTzp0tr3on7ySLRp4PUesc1odyELnkznnsy'
accessTokenSecret <- 'OzgNs8hh2HWPdRYoFwgYiK3Hs8OzOtVNoy5eGFcgaMofs'
setup_twitter_oauth(consumerKey, consumerSecret, accessToken, accessTokenSecret)
library("twitteR", lib.loc="/Library/Frameworks/R.framework/Versions/3.2/Resources/library")
setup_twitter_oauth(consumerKey, consumerSecret, accessToken, accessTokenSecret)
remove(consumerKey, consumerSecret, accessToken, accessTokenSecret)
)
searchTerms <- "Dow Jones"
maxTweets <- 3000
startDate <- '2015-09-28'
endDate <- '2015-09-20'
language <- "en"
resultSort <- 'recent'
tweets <- searchTwitter(searchTerms, n = maxTweets, since = startDate, lang = language, resultType = resultSort)
tweets
tw <- twListToDF(tweets)
View(tw)
searchTwitter
library("rjson", lib.loc="/Library/Frameworks/R.framework/Versions/3.2/Resources/library")
install.packages("json")
maxTweets = 2000
tweets <- searchTwitter(searchTerms, n = maxTweets, since = startDate, lang = language, resultType = resultSort)
searchTwitter.doRppAPICall
doRppAPICall
whos
who
what
getwd()
load("multilevelPSA")
library("multilevelPSA", lib.loc="/Library/Frameworks/R.framework/Versions/3.2/Resources/library")
clear
clc
rm(list=ls())                         # clean up workspace
print(load("data_dow.Rda"))           # taken from your data_dow.RData
print(load("data_dow.Rda"))           # taken from your data_dow.RData
load("/Volumes/Mac OS Drive/Documents/Dropbox/Thesis/R/data_dow.Rda")
str(data_dow)
data_dow$ID <- as.numeric(data_dow$ID)
data_dow$myDate <- as.Date(data_dow$Date) # I need days only
dowMentioned <- function(x, string="dow") string %in% unlist(strsplit(x, " " ))
data_dow$mentioned <- as.numeric(dowMentioned(data_dow$Tweet, "dow")) # 'dow' always mentioned (as expected)
(dm1 <- table(data_dow$myDate))
dm2 <- aggregate(mentioned ~ myDate, data = data_dow, sum)
rng <- range(dm2$myDate)
library(tseries)
x <- get.hist.quote(start=rng[1], end = rng[2], instrument = "^DJI",
provider = "yahoo", quote="Close")
par(mfrow=c(2,1))
install.packages('latex2exp')
-version
getRversion()
install.packages("latex2exp")
getwd()
x = 1
save(x)
save(x, file = "testx.r")
options(error = recover)
sdf
setwd("/Volumes/Mac OS Drive/Thesis/Source Code/R/Pre-processing/")
y <- read.csv(file = "scratch_file.R", header = TRUE, sep = "\n", quote = "", row.names = NULL, stringsAsFactors = FALSE)
Display(y)
View(y)
?find
rm(y)
lsos()
lsos()
lsos()
head(y, n=5)
head(y, n=5)
y[5,]
?gsub
lsos()
y$transformed6[549:54]
y$transformed6[49:54]
file = "This_is_a_string.txt"
file
file
file[1:3]
rm(file)
lsos()
apap = "This_is_a_string.txt"
apap
substr
nchar(apap)
file_name = "2013-11-01_til_2013-09-05.txt_SA_input.txt"
nchar(file_name)
substr(file_name, 36, nchar(file_name))
substr(file_name, 26, nchar(file_name))
substr(file_name, 26, nchar(file_name))
grep(".", file_name)
grepl
grepl(".", file_name)
lsos()
y$transformed6[1:100]
file_name
substr(file_name, 26, nchar(file_name))
substr(file_name, 1, nchar(file_name))
gregexpr(pattern = "\.?", text = file_name)
gregexpr(pattern = "\\.?", text = file_name)
gregexpr(pattern = "\\.", text = file_name)
x <- gregexpr(pattern = "\\.", text = file_name)
x
x[1]
x[[1]]
x[[1]][1]
substr(file_name, 1, gregexpr(pattern = "\\.", text = file_name)[[1]][1])
substr(file_name, 1, gregexpr(pattern = "\\.", text = file_name)[[1]][
 gregexpr(pattern = "\\.", text = file_name)[[1]][1] - 1
substr(file_name, 1, gregexpr(pattern = "\\.", text = file_name)[[1]][1] - 1)
y$transformed6[1:100]
search()
library("dplyr")
file_name
?dplyr
browseVignettes(package = "dplyr")
lsos()
out_file[1:100]
 y$transformed6[1:100]
head(y$transformed6, n = 20)
head(out_file, n = 20)
str(y$transformed6)
str(out_file)
attr(out_file)
attr(*, out_file)
attr(out_file, which = "names")
str(out_file)
out_file[1:30]
out_file[1:30]
str(indent.str = )
str(in_file)
 library("magrittr")
out_file[1:15]
a <- sapply(in_file, function(x) gsub("&amp;", "&", x))
a[5:10]
head(out_file)
>>>>>>> ffe66895523ed7f67d8760544390319fd4deda55
input_path
